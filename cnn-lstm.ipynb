{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e49a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 4ms/step - accuracy: 0.9081 - loss: 0.2638 - val_accuracy: 0.9188 - val_loss: 0.2109\n",
      "Epoch 2/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - accuracy: 0.9180 - loss: 0.2104 - val_accuracy: 0.9213 - val_loss: 0.2003\n",
      "Epoch 3/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9194 - loss: 0.2047 - val_accuracy: 0.9216 - val_loss: 0.1974\n",
      "Epoch 4/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - accuracy: 0.9209 - loss: 0.1995 - val_accuracy: 0.9214 - val_loss: 0.1955\n",
      "Epoch 5/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 4ms/step - accuracy: 0.9210 - loss: 0.1973 - val_accuracy: 0.9216 - val_loss: 0.1934\n",
      "Epoch 6/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 4ms/step - accuracy: 0.9219 - loss: 0.1956 - val_accuracy: 0.9218 - val_loss: 0.1942\n",
      "Epoch 7/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 4ms/step - accuracy: 0.9210 - loss: 0.1974 - val_accuracy: 0.9211 - val_loss: 0.1936\n",
      "Epoch 8/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 4ms/step - accuracy: 0.9220 - loss: 0.1943 - val_accuracy: 0.9224 - val_loss: 0.1929\n",
      "Epoch 9/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 4ms/step - accuracy: 0.9222 - loss: 0.1943 - val_accuracy: 0.9209 - val_loss: 0.1932\n",
      "Epoch 10/10\n",
      "\u001b[1m8064/8064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - accuracy: 0.9225 - loss: 0.1946 - val_accuracy: 0.9225 - val_loss: 0.1917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9222\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(dataset_path):\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(os.path.join(dataset_path, 'Pre_train_D_1.csv'))  # Adjust file name as needed\n",
    "    # Example: Combine multiple CSVs if needed\n",
    "    # df = pd.concat([pd.read_csv(os.path.join(dataset_path, f)) for f in os.listdir(dataset_path) if f.endswith('.csv')])\n",
    "    \n",
    "    # Preprocess\n",
    "    # Convert hex Arbitration_ID to int, handle non-hex values\n",
    "    def safe_hex_to_int(x):\n",
    "        try:\n",
    "            return int(x, 16) if isinstance(x, str) else x\n",
    "        except (ValueError, TypeError):\n",
    "            return 0  # Default for invalid values\n",
    "    \n",
    "    df['Arbitration_ID'] = df['Arbitration_ID'].apply(safe_hex_to_int)\n",
    "    \n",
    "    # Convert Data (hex string) to array of integers, ensure fixed length (8 bytes)\n",
    "    def hex_to_int_array(hex_str):\n",
    "        try:\n",
    "            if isinstance(hex_str, str) and hex_str.strip():\n",
    "                # Remove spaces and convert hex to list of integers\n",
    "                hex_str = hex_str.replace(' ', '')\n",
    "                data = [int(hex_str[i:i+2], 16) for i in range(0, len(hex_str), 2)]\n",
    "                # Pad with zeros or truncate to 8 bytes\n",
    "                if len(data) < 8:\n",
    "                    data.extend([0] * (8 - len(data)))\n",
    "                return data[:8]  # Ensure exactly 8 bytes\n",
    "            return [0] * 8  # Default for NaN, empty, or invalid\n",
    "        except (ValueError, TypeError):\n",
    "            return [0] * 8  # Fallback for any parsing errors\n",
    "    \n",
    "    df['Data'] = df['Data'].apply(hex_to_int_array)\n",
    "    # Convert to numpy array, ensuring shape (n_samples, 8)\n",
    "    data_array = np.array(df['Data'].tolist())  # Shape: (n_samples, 8)\n",
    "    \n",
    "    # Verify shape consistency\n",
    "    if data_array.shape[1] != 8:\n",
    "        raise ValueError(f\"Data array has inconsistent shape: {data_array.shape}\")\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    arbitration_id = scaler.fit_transform(df[['Arbitration_ID']].values)\n",
    "    dlc = scaler.fit_transform(df[['DLC']].values)\n",
    "    timestamp_diff = scaler.fit_transform(df['Timestamp'].diff().fillna(0).values.reshape(-1, 1))\n",
    "    \n",
    "    # Combine features: Timestamp diff, Arbitration_ID, DLC, Data (8 bytes)\n",
    "    X = np.concatenate([timestamp_diff, arbitration_id, dlc, data_array], axis=1)  # Shape: (n_samples, 11)\n",
    "    \n",
    "    # Encode labels (Class: Normal/Attack)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['Class'])\n",
    "    y = to_categorical(y)  # One-hot encode for multi-class\n",
    "    \n",
    "    # Create sequences for LSTM (window of 20 messages)\n",
    "    sequence_length = 20\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_seq.append(X[i:i+sequence_length])\n",
    "        y_seq.append(y[i+sequence_length-1])  # Label for last message in sequence\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq), label_encoder\n",
    "\n",
    "# Build CNN-LSTM model\n",
    "def build_cnn_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # CNN layers\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        # LSTM layers\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Dataset path\n",
    "    dataset_path = \"/Users/bodapati/Downloads/Car_Hacking_Challenge_Dataset_rev20Mar2021/0_Preliminary/0_Training\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y, label_encoder = load_data(dataset_path)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build model\n",
    "    input_shape = (X.shape[1], X.shape[2])  # (sequence_length, num_features)\n",
    "    num_classes = y.shape[1]\n",
    "    model = build_cnn_lstm_model(input_shape, num_classes)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save('cnn_lstm_car_hacking.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
